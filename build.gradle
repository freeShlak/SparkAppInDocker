plugins {
    id 'java'
    id 'application'
    id 'com.bmuschko.docker-remote-api' version "6.1.1"
}

group 'tsibenko'
version '1.0-SNAPSHOT'

sourceCompatibility = 1.8

repositories {
    mavenCentral()
    maven {
        url 'https://dl.bintray.com/cuba-platform/main'
    }
}

wrapper {
    gradleVersion = '6.0.1'
}

dependencies {
    compile group: 'org.apache.spark', name: 'spark-sql_2.12', version: '2.4.0'
    compile group: 'org.apache.lucene.morphology', name: 'russian', version: '1.1'
}

/*================================Creating-Executable-JAR=============================*/

def mainClass = 'Main'

// Building executable jar
application {
    mainClassName = mainClass
}
jar {
    manifest {
        attributes "Main-Class": mainClass
    }

    from {
        configurations.compile.collect { it.isDirectory() ? it : zipTree(it) }
    }

    exclude 'META-INF/*.RSA', 'META-INF/*.SF', 'META-INF/*.DSA'
}

/*================================Creating-Docker-Container=============================*/

// Using plugin to setup docker container with application
import com.bmuschko.gradle.docker.tasks.image.*
import com.bmuschko.gradle.docker.tasks.container.*

// Directory with template files for building docker container
def dockerBuildDir = 'build/docker/'

def uniqueContainerName = 'spark-labs'
def imageName = 'tsibenko/spark-labs'
def imageVersion = '1.0-SNAPSHOT'

// Master-node information
def masterName = 'spark-master'
def masterPort = '7077'

def outerJarName = jar.getArchiveFileName().get()
def innerJarName = 'spark-labs.jar'
def innerAppDirectory = '/app'

def dictionaryName = "emo_dict.csv"

def rootDirect = rootDir.getAbsolutePath()

// Creating DockerFile
task createDockerfile(type: Dockerfile) {
    destFile.set(project.file("$dockerBuildDir/Dockerfile"))
    from 'bde2020/spark-submit:2.4.0-hadoop2.8-scala2.12'

    runCommand("mkdir -p $innerAppDirectory")
    runCommand("mkdir -p $innerAppDirectory/input")
    runCommand("mkdir -p $innerAppDirectory/output")

    copyFile(outerJarName, "$innerAppDirectory/$innerJarName")
    copyFile(dictionaryName, "$innerAppDirectory/$dictionaryName")

    environmentVariable("ENABLE_INIT_DAEMON", "false")
    environmentVariable("SPARK_APPLICATION_JAR_NAME", innerJarName.replace(".jar", ""))
    environmentVariable("SPARK_APPLICATION_JAR_LOCATION", "$innerAppDirectory/$innerJarName")
    environmentVariable("SPARK_APPLICATION_MAIN_CLASS", mainClass)
    environmentVariable("SPARK_MASTER_NAME", masterName)
    environmentVariable("SPARK_MASTER_PORT", masterPort)
    environmentVariable("SPARK_SUBMIT_ARGS", "--conf spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8")

    environmentVariable("DICTIONARY_NAME", dictionaryName)

    workingDir(innerAppDirectory)
    defaultCommand '/bin/bash', "/submit.sh"
}

// Copying created JAR file to tmp directory
task syncJar(type: Copy) {
    dependsOn assemble
    from jar.getArchiveFile()
    into dockerBuildDir
}

task syncDictionary(type: Copy) {
    dependsOn assemble
    from "$rootDirect/$dictionaryName"
    into dockerBuildDir
}

task stopContainer(type: DockerStopContainer) {
    targetContainerId("$uniqueContainerName")
    onError { exc ->
        if (exc.message!=null && !exc.message.contains('No such container')) {
            throw new TaskExecutionException(stopContainer, exc)
        }
    }
}

task removeContainer(type: DockerRemoveContainer) {
    dependsOn stopContainer
    targetContainerId("$uniqueContainerName")
    onError { exc ->
        if (exc.message!=null && !exc.message.contains('No such container')) {
            throw new TaskExecutionException(removeContainer, exc)
        }
    }
}

task removeImage(type: DockerRemoveImage) {
    dependsOn removeContainer
    targetImageId("$imageName:$imageVersion")
    onError { exc ->
        if (exc.message!=null && !exc.message.contains('NotModifiedException')) {
            throw new TaskExecutionException(removeImage, exc)
        }
    }
}

task buildImage(type: DockerBuildImage) {
    dependsOn createDockerfile, syncJar, syncDictionary
    inputDir.set(project.file(dockerBuildDir))
    images.set(["$imageName:$imageVersion"])
}

task createContainer(type: DockerCreateContainer) {
    dependsOn buildImage, removeContainer
    targetImageId buildImage.getImageId()
    containerName.set(uniqueContainerName)
    hostConfig.network.set("spark-labs_default")
    hostConfig.binds = [
            "$rootDirect/input":"$innerAppDirectory/input/",
            "$rootDirect/output":"$innerAppDirectory/output/"
    ]
}

task startContainer(type: DockerStartContainer) {
    dependsOn createContainer
    targetContainerId("$uniqueContainerName")
}

/*========================================The-end========================================*/